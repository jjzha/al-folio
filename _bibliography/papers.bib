---
---

@inproceedings{barrett-etal-2024-domain,
  title={{C}an {H}umans {I}dentify {D}omains?},
  author={Barrett*, Maria and M{\"u}ller-Eberstein*, Max and Bassignana*, Elisa and Brogaard Pauli*, Amalie and Zhang*, Mike and van der Goot*, Rob},
  abstract={Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in TGeGUM: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annotations, allowing us to examine the level of human disagreement and the relative difficulty of each annotation task. With a Fleiss' kappa of at most 0.53 on the sentence level and 0.66 at the prose level, it is evident that despite the ubiquity of domains in NLP, there is little human consensus on how to define them. By training classifiers to perform the same task, we find that this uncertainty also extends to NLP models.},
  booktitle={The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation},
  year={2024},
  abbr={LREC-COLING 2024},
  arxiv={},
  code={bitbucket.org/robvanderg/humans-and-domains},
}

@inproceedings{zhang-2024-compjob,
  title={{C}omputational {J}ob {M}arket {A}nalysis with {N}atural {L}anguage {P}rocessing},
  author={Zhang, Mike},
  abstract={Recent technological advances underscore the dynamic nature of the labor market. These transformative shifts yield significant consequences for employment prospects, resulting in the increase of job vacancy data across platforms and languages. The aggregation of such data holds the potential to gain valuable insights into labor market demands, the emergence of new skills, and the overall facilitation of job matching. These benefits extend to various parties, including job platforms, recruitment agencies, applicants, and other stakeholders within the ecosystem. However, despite the prevalence of such insights in the private sector, we lack transparent language technology systems and data for this domain.
  The primary objective of this thesis is to investigate the use of Natural Language Processing (NLP) technology for the extraction of relevant information from job descriptions. We identify several general challenges within this domain. These encompass a scarcity of available training and evaluation data, a lack of standardized guidelines to annotate data, and a shortage of effective methods for extracting information from job ads.
  Therefore, we embark on a comprehensive study of the entire process: First, framing the problem and getting annotated data for training NLP models. Here, our contributions encompass job description datasets, including a de-identification dataset, and a novel active learning algorithm designed for efficient model training. Second, we introduce several extraction methodologies to tackle the task of information extraction from job advertisement data: A skill extraction approach using weak supervision, a taxonomy-aware pre-training methodology adapting a multilingual language model to the job market domain, and a retrieval-augmented model leveraging multiple skill extraction datasets to enhance overall extraction performance. Lastly, given the extracted information, we delve into the grounding of this data within a designated taxonomy.},
  booktitle={Ph.D. Thesis},
  year={2024},
  abbr={Ph.D. Thesis},
  isbn={978-87-7949-414-5},
  slides={thesis-presentation-mz.pdf},
  pdf={https://pure.itu.dk/ws/portalfiles/portal/103296564/PhD_Thesis_Temporary_Version_Mike_Zhang.pdf}
}

@inproceedings{singh-etal-2024-aya-dataset,
  title={{A}ya {D}ataset: An Open-Access Collection for Multilingual Instruction Tuning},
  author={Singh, Shivalika and Vargus, Freddie and Karlsson, Börje F. and Mahendiran, Abinaya and Ko, Wei-Yin and Shandilya, Herumb and Patel, Jay and Mataciunas, Deividas and OMahony, Laura and Zhang, Mike and Hettiarachchi, Ramith and Wilson, Joseph and Machado, Marina and Souza Moura, Luisa and Krzemiński, Dominik and Fadaei, Hakimeh and Ergün, Irem and Okoh, Ifeoma and Alaagib, Aisha and Mudannayake, Oshan and Alyafeai, Zaid and Chien, Vu Minh and Ruder, Sebastian and Guthikonda, Surya and Alghamdi, Emad A. and Gehrmann, Sebastian and Muennighoff, Niklas and Bartolo, Max and Kreutzer, Julia and Üstün, Ahmet and Fadaee, Marzieh and Hooker, Sara},
  abstract={Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as a valuable framework for future research collaborations that aim to bridge gaps in resources.},
  booktitle={arXiv},
  year={2024},
  abbr={ArXiv},
  arxiv={2402.06619},
  code={https://huggingface.co/datasets/CohereForAI/aya_dataset},
}


@inproceedings{zhang-etal-2024-nnose,
  title={{NNOSE}: Nearest Neighbor Occupational Skill Extraction},
  author={Zhang, Mike and Goot, Rob van der and Kan, Min-Yen and Plank, Barbara},
  abstract={The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text. With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well. We tackle the complexity in occupational skill datasets tasks—combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets. In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner. Our proposed method, Nearest Neighbor Occupational Skill Extraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore. This improves skill extraction without additional fine-tuning. Crucially, we observe a performance gain in predicting infrequent patterns, with substantial gains of up to 30% span-F1 in cross-dataset settings.},
  booktitle={The 18th Conference of the European Chapter of the Association for Computational Linguistics},
  year={2024},
  abbr={EACL 2024},
  arxiv={2401.17092},
  code={https://github.com/mainlp/nnose},
  selected={true}
}

@inproceedings{zhang-etal-2024-eljob,
  title={Entity Linking in the Job Market Domain},
  author={Zhang, Mike and Goot, Rob van der and Plank, Barbara},
  abstract={In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain. Disambiguating skill mentions can help us get insight into the current labor market demands. In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014). Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill. In this work, we link more fine-grained span-level mentions of skills. We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention--skill pair dataset and evaluate them on a human-annotated skill-linking benchmark. Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE performs better in loose evaluation (accuracy@k).},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  year={2024},
  arxiv={2401.17979},
  code={https://github.com/mainlp/el_esco},
  abbr={EACL 2024},
}

@inproceedings{senger-etal-2024-survey,
  title={Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction and Classification from Job Postings},
  author={Senger, Elena and Zhang, Mike and Goot, Rob van der and Plank, Barbara},
  abstract={Recent years have brought significant advances to Natural Language Processing (NLP), which enabled fast progress in the field of computational job market analysis. Core tasks in this application domain are skill extraction and classification from job postings. Because of its quick growth and its interdisciplinary nature, there is no exhaustive assessment of this emerging field. This survey aims to fill this gap by providing a comprehensive overview of deep learning methodologies, datasets, and terminologies specific to NLP-driven skill extraction and classification. Our comprehensive cataloging of publicly available datasets addresses the lack of consolidated information on dataset creation and characteristics. Finally, the focus on terminology addresses the current lack of consistent definitions for important concepts, such as hard and soft skills, and terms relating to skill extraction and classification.},
  booktitle={The 1st Workshop on Natural Language Processing for Human Resources (EACL Workshop)},
  year={2024},
  arxiv={2402.05617},
  abbr={NLP4HR 2024}
}

@inproceedings{magron-etal-2024-jobskape,
  title={JobSkape: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching},
  author={Magron, Antoine and Dai, Anna and Zhang, Mike and Montariol, Syrielle, and Bosselut, Antoine},
  booktitle={The 1st Workshop on Natural Language Processing for Human Resources (EACL Workshop)},
  abstract={Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.},
  year={2024},
  arxiv={2402.03242},
  code={https://github.com/magantoine/JobSkape},
  abbr={NLP4HR 2024}
}

@inproceedings{nguyen-etal-2024-rethinking,
  title={Rethinking Skill Extraction in the Job Market Domain using Large Language Models},
  author={Nguyen, Khanh Cao and Zhang, Mike and Montariol, Syrielle, and Bosselut, Antoine},
  abstract={Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences. We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.},
  booktitle={The 1st Workshop on Natural Language Processing for Human Resources (EACL Workshop)},
  year={2024},
  arxiv={2402.03832},
  code={https://github.com/epfl-nlp/SCESC-LLM-skill-extraction},
  abbr={NLP4HR 2024}
}

@inproceedings{zhang-etal-2023-escoxlm,
  title={ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain},
  author={Zhang, Mike and Goot, Rob van der and Plank, Barbara},
  booktitle={Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics},
  abstract={The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-R, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves state-of-the-art results on 6 out of 9 datasets. Our analysis reveals that ESCOXLM-R performs better on short spans and outperforms XLM-R on entity-level and surface-level span-F1, likely due to ESCO containing short skill and occupation titles, and encoding information on the entity-level.},
  year={2023},
  abbr={ACL 2023},
  selected={true},
  pdf={https://aclanthology.org/2023.acl-long.662/},
  code={https://github.com/mainlp/escoxlmr}
}

@inproceedings{bassignana2022evidence,
  title={Evidence > Intuition: Transferability Estimation for Encoder Selection},
  author={Bassignana*, Elisa and M{\"u}ller-Eberstein*, Max and Zhang*, Mike and Plank, Barbara},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  abstract={With the increase in availability of large pre-trained language models (LMs) in Natural Language Processing (NLP), it becomes critical to assess their fit for a specific target task a priori - as fine-tuning the entire space of available LMs is computationally prohibitive and unsustainable. However, encoder transferability estimation has received little to no attention in NLP. In this paper, we propose to generate quantitative evidence to predict which LM, out of a pool of models, will perform best on a target task without having to fine-tune all candidates. We provide a comprehensive study on LM ranking for 10 NLP tasks spanning the two fundamental problem types of classification and structured prediction. We adopt the state-of-the-art Logarithm of Maximum Evidence (LogME) measure from Computer Vision (CV) and find that it positively correlates with final LM performance in 94% of the setups. In the first study of its kind, we further compare transferability measures with the de facto standard of human practitioner ranking, finding that evidence from quantitative metrics is more robust than pure intuition and can help identify unexpected LM candidates.},
  year={2022},
  abbr={EMNLP 2022},
  pdf={https://aclanthology.org/2022.emnlp-main.283.pdf},
  code={https://github.com/mainlp/logme-nlp}
}

@inproceedings{ulmer2022standards,
  title={Experimental Standards for Deep Learning in Natural Language Processing Research},
  author={Ulmer, Dennis and Bassignana, Elisa and M{\"u}ller-Eberstein, Max and Varab, Daniel and Zhang, Mike and Goot, Rob van der and Hardmeier, Christian and Plank, Barbara},
  abstract={The field of Deep Learning (DL) has undergone explosive growth during the last decade, with a substantial impact on Natural Language Processing (NLP) as well. Yet, compared to more established disciplines, a lack of common experimental standards remains an open challenge to the field at large. Starting from fundamental scientific principles, we distill ongoing discussions on experimental standards in NLP into a single, widely-applicable methodology. Following these best practices is crucial to strengthen experimental evidence, improve reproducibility and support scientific progress. These standards are further collected in a public repository to help them transparently adapt to future needs.},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  year={2022},
  abbr={EMNLP 2022},
  pdf={https://aclanthology.org/2022.findings-emnlp.196.pdf},
  code={https://github.com/Kaleidophon/awesome-experimental-standards-deep-learning}
}

@inproceedings{zhang2022skill,
  title={Skill Extraction from Job Postings using Weak Supervision},
  author={Zhang, Mike and Jensen, Kristian N{\o}rgaard and Goot, Rob van der and Plank, Barbara},
  booktitle={RecSys in HR'22: The 2nd Workshop on Recommender Systems for Human Resources, in conjunction with the 16th ACM Conference on Recommender Systems, September 18--23, 2022, Seattle, USA.},
  abstract={Aggregated data obtained from job postings provide powerful insights into labor market demands, and emerging skills, and aid job matching. However, most extraction approaches are supervised and thus need costly and time-consuming annotation. To overcome this, we propose Skill Extraction with Weak Supervision. We leverage the European Skills, Competences, Qualifications and Occupations taxonomy to find similar skills in job ads via latent representations. The method shows a strong positive signal, outperforming baselines based on token-level and syntactic patterns.},
  year={2022},
  organization={CEUR Workshop Proceedings},
  abbr={RecSys HR'22},
  pdf={https://ceur-ws.org/Vol-3218/RecSysHR2022-paper_10.pdf},
  code={https://github.com/jjzha/skill-extraction-weak-supervision}
}

@inproceedings{zhang-etal-2022-skillspan,
    title = "{S}kill{S}pan: Hard and Soft Skill Extraction from {E}nglish Job Postings",
    author = "Zhang*, Mike  and
      Jensen*, Kristian  and
      Sonniks, Sif  and
      Plank, Barbara",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    abstract = "Skill Extraction (SE) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different sources annotated for hard and soft skills by domain experts. We introduce a BERT baseline (Devlin et al., 2019). To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997). Our results show that the domain-adapted models significantly outperform their non-adapted counterparts, and single-task outperforms multi-task learning.",
    year = "2022",
    pages = "4962--4984",
    selected={true},
    abbr={NAACL 2022},
    pdf={https://aclanthology.org/2022.naacl-main.366.pdf},
    code={https://github.com/kris927b/SkillSpan}
}

@inproceedings{zhang-etal-2022-kompetencer,
    title = "Kompetencer: Fine-grained Skill Classification in {D}anish Job Postings via Distant Supervision and Transfer Learning",
    author = "Zhang*, Mike  and
      Jensen*, Kristian N{\o}rgaard  and
      Plank, Barbara",
    abstract = "Skill Classification (SC) is the task of classifying job competences from job postings. This work is the first in SC applied to Danish job vacancy data. We release the first Danish job posting dataset:* Kompetencer*(_en_: competences), annotated for nested spans of competences. To improve upon coarse-grained annotations, we make use of The European Skills, Competences, Qualifications and Occupations (ESCO; le Vrang et al.,(2014)) taxonomy API to obtain fine-grained labels via distant supervision. We study two setups: The zero-shot and few-shot classification setting. We fine-tune English-based models and RemBERT (Chung et al., 2020) and compare them to in-language Danish models. Our results show RemBERT significantly outperforms all other models in both the zero-shot and the few-shot setting.",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    year = "2022",
    pages = "436--447",
    abbr={LREC 2022},
    pdf={https://aclanthology.org/2022.lrec-1.46.pdf},
    code={https://github.com/jjzha/kompetencer}
}

@article{ulmer2022experimental,
  title={Experimental Standards for Deep Learning Research: A Natural Language Processing Perspective},
  author={Ulmer, Dennis and Bassignana, Elisa and M{\"u}ller-Eberstein, Max and Varab, Daniel and Zhang, Mike and Hardmeier, Christian and Plank, Barbara},
  journal={arXiv preprint arXiv:2204.06251},
  year={2022},
  abbr={SMILES 2022}
}

@inproceedings{zhang2021cartography,
  title={Cartography Active Learning},
  author={Zhang, Mike and Plank, Barbara},
  abstract={We propose Cartography Active Learning (CAL), a novel Active Learning (AL) algorithm that exploits the behavior of the model on individual instances during training as a proxy to find the most informative instances for labeling. CAL is inspired by data maps, which were recently proposed to derive insights into dataset quality (Swayamdipta et al., 2020). We compare our method on popular text classification tasks to commonly used AL strategies, which instead rely on post-training behavior. We demonstrate that CAL is competitive to other common AL methods, showing that training dynamics derived from small seed data can be successfully used for AL. We provide insights into our new AL method by analyzing batch-level statistics utilizing the data maps. Our results further show that CAL results in a more data-efficient learning strategy, achieving comparable or better results with considerably less training data.},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={395--406},
  year={2021},
  abbr={EMNLP 2021},
  pdf={https://aclanthology.org/2021.findings-emnlp.36v2.pdf},
  code={https://github.com/jjzha/cartography-al}
}

@inproceedings{jensen2021identification,
  title={De-identification of Privacy-related Entities in Job Postings},
  author={Jensen*, Kristian N{\o}rgaard and Zhang*, Mike and Plank, Barbara},
  booktitle={Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},
  abstract={De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data. It has been well-studied within the medical domain. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains. In this paper, we focus on job postings. We present JobStack, a new corpus for de-identification of personal data in job vacancies on Stackoverflow. We introduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer models. To improve upon these baselines, we experiment with contextualized embeddings and distantly related auxiliary data via multi-task learning. Our results show that auxiliary data improves de-identification performance. Surprisingly, vanilla BERT turned out to be more effective than a BERT model trained on other portions of Stackoverflow.},
  pages={210--221},
  year={2021},
  abbr={NoDaLiDa 2021},
  pdf={https://aclanthology.org/2021.nodalida-main.21/},
  code={https://github.com/kris927b/JobStack}
}

@inproceedings{zhang2019effect,
  title={The Effect of Translationese in Machine Translation Test Sets},
  author={Zhang, Mike and Toral, Antonio},
  abstract={The effect of translationese has been studied in the field of machine translation (MT), mostly with respect to training data. We study in depth the effect of translationese on test data, using the test sets from the last three editions of WMT's news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction.},
  booktitle={Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)},
  pages={73--81},
  year={2019},
  abbr={WMT 2019},
  pdf={https://aclanthology.org/W19-5208/},
  code={https://github.com/jjzha/translationese}
}

@inproceedings{zhang2019grunn2019,
  title={Grunn2019 at SemEval-2019 task 5: Shared task on multilingual detection of hate},
  author={Zhang, Mike and David, Roy and Graumans, Leon and Timmerman, Gerben},
  booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},
  abstract={Hate speech occurs more often than ever and polarizes society. To help counter this polarization, SemEval 2019 organizes a shared task called the Multilingual Detection of Hate. The first task (A) is to decide whether a given tweet contains hate against immigrants or women, in a multilingual perspective, for English and Spanish. In the second task (B), the system is also asked to classify the following sub-tasks: hateful tweets as aggressive or not aggressive, and to identify the target harassed as individual or generic. We evaluate multiple models, and finally combine them in an ensemble setting. This ensemble setting is built of five and three submodels for the English and Spanish task respectively. In the current setup it shows that using a bigger ensemble for English tweets performs mediocre, while a slightly smaller ensemble does work well for detecting hate speech in Spanish tweets. Our results on the test set for English show 0.378 macro F1 on task A and 0.553 macro F1 on task B. For Spanish the results are significantly higher, 0.701 macro F1 on task A and 0.734 macro F1 for task B.},
  pages={391--395},
  year={2019},
  abbr={SemEval 2019},
  pdf={https://aclanthology.org/S19-2069/}
}